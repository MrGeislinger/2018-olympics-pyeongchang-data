{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import csv\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "\n",
    "I'm scraping the data from https://www.olympic.org/ via BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful references to directories (all directories end with '/')\n",
    "dr_connector = 'en/'\n",
    "dr_imgs = 'resOWG2018/img/'\n",
    "dr_results = 'https://www.olympic.org/pyeongchang-2018/results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sports & Events Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference page to list of all sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schedule page has reference to all sports in table\n",
    "url_schedule = 'https://www.olympic.org/pyeongchang-2018/results/en/general/competition-schedule.htm'\n",
    "\n",
    "# Get document to be passed in for soup (better/cleaner practice)\n",
    "request_schedule = requests.get(url_schedule)\n",
    "text_schedule = request_schedule.text\n",
    "\n",
    "# Get all sports from schedule page\n",
    "soup_schedule = BeautifulSoup(text_schedule, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image container and the name/link container (comes in pairs)\n",
    "sports = soup_schedule.find_all('td', {'class':['disciplinePicture', 'styleLeft']})\n",
    "\n",
    "# Dictonary for the sports\n",
    "sports_info = []\n",
    "\n",
    "# Go every other since it always matches to one sport\n",
    "for img, name in zip(sports[::2], sports[1::2]):\n",
    "    \n",
    "    # Skip for the ceremony image (and other errors)\n",
    "    if name.a == None:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Get the image link which has the sports ID \n",
    "    # form: ../../resOWG2018/img/sports/CER.png\n",
    "    sport_img_link = img.img['src']\n",
    "    \n",
    "    # Get ID from link\n",
    "    match = re.search('(\\w+)\\.png$', sport_img_link)\n",
    "    sport_id = match.group(1)\n",
    "    \n",
    "    # Get image as a link\n",
    "    sport_img = '{}{}sports/{}.png'.format(dr_results, dr_imgs, sport_id)\n",
    "    \n",
    "    \n",
    "    # Get sport's schedule page\n",
    "    match = re.search('(([-\\w]+)\\/daily-schedule.htm)$', name.a['href'])    \n",
    "    sport_schedule = '{}{}{}'.format(dr_results, dr_connector, match.group(1))\n",
    "\n",
    "    # Get sport's full name from link (words separated by -)\n",
    "    sport_name = match.group(2)\n",
    "    \n",
    "    sport_dict = {'id': sport_id, 'img': sport_img, 'schedule': sport_schedule, 'name': sport_name}\n",
    "    sports_info.append(sport_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "print(len(sports_info))\n",
    "\n",
    "for sport in sports_info:\n",
    "    for k, v in sport.items():\n",
    "        print(k,v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference page to sports' different events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each sport, get the different events\n",
    "# Save all event info into a list of events for the sport\n",
    "events_info = []\n",
    "\n",
    "for sport in sports_info:\n",
    "    sport_id = sport['id']\n",
    "    sport_name = sport['name']\n",
    "    \n",
    "    # Get HTML text from sport's list of events\n",
    "    url_event = '{}{}{}/sport-entries.htm'.format(dr_results, dr_connector, sport_name)\n",
    "    request_event = requests.get(url_event)\n",
    "    text_event = request_event.text\n",
    "    soup_event = BeautifulSoup(text_event, 'html.parser')\n",
    "    \n",
    "    # Look for all events for this sport\n",
    "    events = soup_event.find_all('li', class_='entriesByEventElem')\n",
    "    \n",
    "    for event in events:\n",
    "        # Get info from event page\n",
    "        event_page_link = event.a['href']\n",
    "        match = re.search('\\/(entries-by-event-([\\w-]*)\\.htm)$', event_page_link)\n",
    "        \n",
    "        # Get the web page for the event \n",
    "        event_page = '{}{}{}/{}'.format(dr_results, dr_connector, sport_name, match.group(1))\n",
    "        event_name = match.group(2)\n",
    "        \n",
    "        # Process name so it is easier for reading\n",
    "        event_readable = event.a.text.strip().lower()\n",
    "        \n",
    "        # Get sex by seeing if it's men's, women's, or mixed (definitions checked)\n",
    "        sex_categories = {'mixed':'mixed', 'gundersen':'men', 'man':'men', 'men':'men', 'women':'women', 'ladies':'women'}\n",
    "        # Default to mixed event\n",
    "        event_sex = 'mixed'\n",
    "        is_assigned = False\n",
    "        # Loop over each category (time consuming but necessary)\n",
    "        for sex in sex_categories.keys():\n",
    "            \n",
    "            # Check if any of the words is the sex-term\n",
    "            if sex in event_name.split('-'):\n",
    "                # If there was more than one label applied, it's a mixed event \n",
    "                if is_assigned:\n",
    "                    event_sex = 'mixed'\n",
    "                    break\n",
    "                \n",
    "                event_sex = sex_categories[sex]\n",
    "                is_assigned = True       \n",
    "\n",
    "        \n",
    "        # Save event info into list of events (for this sport)\n",
    "        event_info = {'name': event_name, 'sport_id':sport_id, 'sex': event_sex, 'readable_name': event_readable, 'page': event_page}\n",
    "        events_info.append(event_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for events\n",
    "len(events_info)\n",
    "for i in range(5):\n",
    "    for k,v in events_info[i].items():\n",
    "        print(k,v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference events' pages to get athlete info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to make this a constant outsided of method (reusing this)\n",
    "convert_month_to_number = {\n",
    "    'Jan': '01',\n",
    "    'Feb': '02',\n",
    "    'Mar': '03',\n",
    "    'Apr': '04',\n",
    "    'May': '05',\n",
    "    'Jun': '06',\n",
    "    'Jul': '07',\n",
    "    'Aug': '08',\n",
    "    'Sep': '09',\n",
    "    'Oct': '10',\n",
    "    'Nov': '11',\n",
    "    'Dec': '12' \n",
    "}\n",
    "\n",
    "def get_athlete_profile_info(athlete_profile_page):\n",
    "    # Return a dictionary of other info\n",
    "    info = {}\n",
    "    \n",
    "    ## Get document to be passed in for soup (better/cleaner practice)\n",
    "    request_profile = requests.get(athlete_profile_page)\n",
    "    text_profile = request_profile.text\n",
    "    soup_profile = BeautifulSoup(text_profile, 'html.parser')\n",
    "    \n",
    "    # Get birthdate\n",
    "    quick_info_div = soup_profile.body.findAll(text=re.compile('Birth Date:'), limit=1)[0].parent.parent\n",
    "    birthdate_text = quick_info_div.text\n",
    "    match = re.search('(\\d{1,2}) (\\w+) (\\d{4})', birthdate_text)\n",
    "\n",
    "    # Day will be two digit number\n",
    "    info['birth_day'] =  match.group(1)\n",
    "    # Change month text to two digit number\n",
    "    info['birth_month'] = convert_month_to_number[match.group(2)]\n",
    "    info['birth_year'] = match.group(3)\n",
    "    \n",
    "    return(info)\n",
    "    \n",
    "    \n",
    "\n",
    "# Create a way to pull data from athlete's div\n",
    "def get_athlete_info(athlete_div):\n",
    "    # Country is 3 country code\n",
    "    athlete_country = athlete_div['attrcountrycode']\n",
    "\n",
    "    # picture is within another div\n",
    "    athlete_photo_div = athlete_div.find_all('div', class_='playerTagContainerPhoto')[0]\n",
    "    athlete_photo = athlete_photo_div.img['src']\n",
    "    # Create URL for photo\n",
    "    match = re.search('\\.\\./\\.\\./(.*)$', athlete_photo)\n",
    "    athlete_photo = match.group(1)\n",
    "    athlete_photo = '{}{}'.format(dr_results, athlete_photo)\n",
    "\n",
    "    # ID is numbers from picture name\n",
    "    athlete_id = re.search('\\/(\\d+)\\..*$', athlete_photo).group(1)\n",
    "\n",
    "    # name & link to profile\n",
    "    athlete_profile_link = athlete_div.find_all('div', class_='nameLine')[0].a\n",
    "    athlete_name = athlete_profile_link.text.lower()\n",
    "\n",
    "    # Create URL for profile page\n",
    "    match = re.search('\\.\\./\\.\\./en/(.*)$', athlete_profile_link['href'])\n",
    "    athlete_page = match.group(1)\n",
    "    athlete_page = '{}{}{}'.format(dr_results, dr_connector, athlete_page)\n",
    "    \n",
    "    \n",
    "    # Create dictionary for athlete\n",
    "    athlete_dict = {\n",
    "        'id':athlete_id, \n",
    "        'name':athlete_name, \n",
    "        'country_id':athlete_country, \n",
    "        'photo':athlete_photo, \n",
    "        'profile':athlete_page\n",
    "    }\n",
    "    \n",
    "    return(athlete_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make rankings table for event\n",
    "## TODO: get result of event for each entry (may not be avail)\n",
    "\n",
    "# Keep athlete data in its own table\n",
    "athletes_info = []\n",
    "\n",
    "## go by each event \n",
    "for event in events_info:\n",
    "    print(event['page'])\n",
    "    \n",
    "    # Use the entries page with all athlete entries\n",
    "    entries_page = event['page']\n",
    "    \n",
    "    # Get document to be passed in for soup (better/cleaner practice)\n",
    "    request_entries = requests.get(entries_page)\n",
    "    text_entries = request_entries.text\n",
    "    soup_entries = BeautifulSoup(text_entries, 'html.parser')\n",
    "\n",
    "    # Get all athlete entries\n",
    "    athlete_divs = soup_entries.find_all('div', class_='playerTagSmallBox')\n",
    "    \n",
    "    # Count number of athletes/entries\n",
    "    num_athletes = 0\n",
    "    num_athletes_unique = 0\n",
    "    num_entries = soup_entries.find_all('div', class_='ResCaption')[-1].text\n",
    "\n",
    "    # Try to get number of enties\n",
    "    \n",
    "    try:\n",
    "        match = re.search('(\\d+)\\sEntries', num_entries.strip())\n",
    "        num_entries = match.group(1).strip()\n",
    "    except:\n",
    "        match = re.search('Entries: (\\d+)', num_entries.strip())\n",
    "        num_entries = match.group(1).strip()\n",
    "        \n",
    "    # Look at each athlete entry\n",
    "    for athlete_div in athlete_divs:\n",
    "    \n",
    "        # TODO: Get athlete id first (so we don't have to do work every time)\n",
    "            \n",
    "        athlete_dict = get_athlete_info(athlete_div)\n",
    "\n",
    "        num_athletes += 1\n",
    "        \n",
    "        # Check athlete doesn't already exist in athlete table by checking id\n",
    "        if athlete_dict['id'] in [athlete['id'] for athlete in athletes_info]:\n",
    "            continue\n",
    "            \n",
    "        # Add athletes into event and incr counter\n",
    "        num_athletes_unique += 1\n",
    "        athletes_info.append(athlete_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for events\n",
    "print(len(athletes_info))\n",
    "\n",
    "print(athletes_info[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get additional athlete info via her profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for athlete in athletes_info: \n",
    "    extra_info = get_athlete_profile_info(athlete['profile'])\n",
    "    # Save into athlete's dictionary\n",
    "    for k,v in extra_info.items():\n",
    "        athlete[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save sports data into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV file from list of sports dictionaries\n",
    "with open('sports.csv', 'w') as sports_csv:\n",
    "    writer = csv.writer(sports_csv)\n",
    "    \n",
    "    # Headers\n",
    "    headers = ['id', 'name', 'img', 'schedule']\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    # Get each dictionary assoc. with the sport\n",
    "    for sport in sports_info:\n",
    "        # Use only the headers (in order) to write row \n",
    "        row = [sport[key] for key in headers]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Country Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference page to list of all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list_countries = 'https://www.olympic.org/pyeongchang-2018/results/en/general/nocs-list.htm'\n",
    "\n",
    "# Get document to be passed in for soup (better/cleaner practice)\n",
    "request_countries = requests.get(url_list_countries)\n",
    "text_countires = request_countries.text\n",
    "\n",
    "# Get all countries from main page\n",
    "soup_countries = BeautifulSoup(text_countires, 'html.parser')\n",
    "countries = soup_countries.find_all('div', class_='CountriesListItem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country pages, flags, IDs, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dictionaries of countries info \n",
    "countries_info = []\n",
    "\n",
    "# Iterate over countries and save info\n",
    "for country in countries:\n",
    "    # Country code gives an identifier of 3 character\n",
    "    country_id = country['attrcountrycode']\n",
    "\n",
    "    # Country web page\n",
    "    country_page_link = country.a['href']\n",
    "    match = re.search('\\/(noc-entries-([-\\w]+)\\.htm)$',country_page_link)\n",
    "    # group(1) form: noc-entries-country.htm\n",
    "    country_page = '{}{}general/{}'.format(dr_results, dr_connector, match.group(1))\n",
    "    \n",
    "    # Country full name\n",
    "    country_name = match.group(2) \n",
    "\n",
    "    # Flag image =>\n",
    "    country_flag = '{}resCOMMON/img/flags/{}.png'.format(dr_results,country_id)\n",
    "    \n",
    "    # Create a dictionary to be saved\n",
    "    country_dict = {'code_id':country_id, 'name':country_name, 'page':country_page, 'flag':country_flag}\n",
    "    countries_info.append(country_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "print(len(countries_info))\n",
    "\n",
    "\n",
    "for key, info in countries_info[0].items():\n",
    "    print(key, info)\n",
    "print()\n",
    "for key, info in countries_info[-1].items():\n",
    "    print(key, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save country data into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV file from list of countries dictionaries\n",
    "with open('countries.csv', 'w') as countries_csv:\n",
    "    writer = csv.writer(countries_csv)\n",
    "    \n",
    "    # Headers\n",
    "    headers = ['code_id', 'name', 'flag', 'page']\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    # Get each dictionary assoc. with the sport\n",
    "    for country in countries_info:\n",
    "        # Use only the headers (in order) to write row \n",
    "        row = [country[key] for key in headers]\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
